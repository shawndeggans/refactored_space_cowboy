{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafcc9c5-125c-474b-a85e-d1088b4524ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Understanding Databricks Tagging Options\n",
    "\n",
    "Databricks provides three distinct tagging systems, each serving different purposes within the platform:\n",
    "\n",
    "1. **Resource-level tagging**: For attributing compute costs to teams, projects, or users\n",
    "2. **Unity Catalog securable object tagging**: For organizing, classifying, and governing data assets\n",
    "3. **Serverless compute workload tagging**: For tracking usage of serverless resources\n",
    "\n",
    "These tagging systems enable better governance, cost management, and organization of both compute resources and data assets within Databricks[^1][^7].\n",
    "\n",
    "## Resource-Level Tagging\n",
    "\n",
    "Resource-level tags allow you to attribute compute usage to specific teams, projects, or cost centers with greater granularity than default tags. These tags propagate to both your account's usage logs and applicable cloud resources[^1][^7].\n",
    "\n",
    "### Types of Resource Tags\n",
    "\n",
    "There are two types of resource tags in Databricks:\n",
    "\n",
    "1. **Default tags**: Automatically applied by Databricks to compute resources, providing basic metadata like vendor, cluster ID, and creator\n",
    "2. **Custom tags**: User-defined tags that you add to resources for more granular tracking[^1][^7]\n",
    "\n",
    "### Supported Resources for Custom Tags\n",
    "\n",
    "You can apply custom tags to the following resources:\n",
    "\n",
    "\n",
    "| Object | Tagging interface | Python approach |\n",
    "| :-- | :-- | :-- |\n",
    "| Workspace | Cloud provider portal | Cloud provider API |\n",
    "| Pool | Pools UI or Instance Pool API | Databricks API |\n",
    "| Clusters (all-purpose and job) | Compute UI or Clusters API | Databricks API |\n",
    "| SQL warehouse | SQL warehouse UI or Warehouses API | Databricks API |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e299e93-c87a-4794-887d-a7cbac6ffa0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8234a52-82fe-430d-8b58-7ab6f6f71185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "    \n",
    "load_dotenv()\n",
    "\n",
    "TOKEN = os.getenv(\"TOKEN\")\n",
    "DATABRICKS_INSTANCE = os.getenv(\"DATABRICKS_INSTANCE\")\n",
    "CLUSTER_ID = os.getenv(\"CLUSTER_ID\")\n",
    "WAREHOUSE_ID = os.getenv(\"WAREHOUSE_ID\")\n",
    "\n",
    "print(f\"TOKEN: {TOKEN}\")\n",
    "print(f\"DATABRICKS_INSTANCE: {DATABRICKS_INSTANCE}\")\n",
    "print(f\"CLUSTER_ID: {CLUSTER_ID}\")\n",
    "print(f\"WAREHOUSE_ID: {WAREHOUSE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16db89f4-16db-405f-a6ec-8974d02ec0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Implementation with Python\n",
    "\n",
    "#### Tagging Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1145f7b3-3f02-4a8f-8af5-a99f0d4ad931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CLUSTER_ID = CLUSTER_ID\n",
    "DATABRICKS_INSTANCE = DATABRICKS_INSTANCE\n",
    "TOKEN = TOKEN\n",
    "TAGS_TO_ADD = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_cluster(cluster_id, custom_tags, databricks_instance, token):\n",
    "    \"\"\"\n",
    "    Add or update tags on an existing Databricks cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_id: ID of the existing cluster\n",
    "    - custom_tags: Dictionary of tag key-value pairs\n",
    "    - databricks_instance: Your Databricks workspace URL\n",
    "    - token: Your Databricks personal access token\n",
    "    \"\"\"\n",
    "    # First, get current cluster configuration\n",
    "    api_endpoint = f\"https://{databricks_instance}/api/2.0/clusters/get\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        api_endpoint,\n",
    "        headers=headers,\n",
    "        params={\"cluster_id\": cluster_id}\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error getting cluster: {response.text}\")\n",
    "        return False\n",
    "    \n",
    "    # Get current configuration\n",
    "    cluster_config = response.json()\n",
    "    \n",
    "    # Update the tags\n",
    "    current_tags = cluster_config.get(\"custom_tags\", {})\n",
    "    current_tags.update(custom_tags)\n",
    "    cluster_config[\"custom_tags\"] = current_tags\n",
    "    \n",
    "    # Remove fields that cannot be included in edit request\n",
    "    for field in [\"creator_user_name\", \"start_time\", \"state\", \n",
    "                 \"state_message\", \"default_tags\", \"cluster_source\"]:\n",
    "        if field in cluster_config:\n",
    "            del cluster_config[field]\n",
    "\n",
    "    # Ensure cluster_id is included in the configuration\n",
    "    cluster_config[\"cluster_id\"] = cluster_id\n",
    "    \n",
    "    # Edit cluster with updated tags\n",
    "    edit_endpoint = f\"https://{databricks_instance}/api/2.0/clusters/edit\"\n",
    "    \n",
    "    response = requests.post(\n",
    "        edit_endpoint,\n",
    "        headers=headers,\n",
    "        data=json.dumps(cluster_config)\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Successfully updated tags on cluster {cluster_id}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Error updating tags: {response.text}\")\n",
    "        return False\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_cluster(\n",
    "    cluster_id=CLUSTER_ID,\n",
    "    custom_tags=TAGS_TO_ADD,\n",
    "    databricks_instance=DATABRICKS_INSTANCE,\n",
    "    token=TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fbd0743-e1ce-4a21-8e55-1fe6dec522b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Creating a New Cluster with Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3589f57d-7d73-415c-910d-a096f005f11d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CLUSTER_NAME = \"My Databricks Cluster\"\n",
    "DATABRICKS_INSTANCE = DATABRICKS_INSTANCE\n",
    "TOKEN = TOKEN\n",
    "SPARK_VERSION = \"11.3.x-scala2.12\"\n",
    "NODE_TYPE = \"Standard_DS3_v2\"\n",
    "MIN_WORKERS = 1\n",
    "MAX_WORKERS = 2\n",
    "TAGS_TO_ADD = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def create_cluster_with_tags(cluster_name, custom_tags, databricks_instance, token, \n",
    "                            spark_version=\"11.3.x-scala2.12\", node_type=\"Standard_DS3_v2\", \n",
    "                            min_workers=1, max_workers=2):\n",
    "    \"\"\"\n",
    "    Create a new Databricks cluster with custom tags.\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_name: Name for the new cluster\n",
    "    - custom_tags: Dictionary of tag key-value pairs\n",
    "    - databricks_instance: Your Databricks workspace URL\n",
    "    - token: Your Databricks personal access token\n",
    "    - spark_version: Databricks Runtime version\n",
    "    - node_type: VM type for the cluster nodes\n",
    "    - min_workers, max_workers: Worker node count range\n",
    "    \"\"\"\n",
    "    api_endpoint = f\"https://{databricks_instance}/api/2.0/clusters/create\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # IMPORTANT: Don't use 'Name' as a custom tag key (reserved by Databricks)\n",
    "    if \"Name\" in custom_tags:\n",
    "        print(\"Warning: 'Name' is a reserved tag and has been removed\")\n",
    "        del custom_tags[\"Name\"]\n",
    "    \n",
    "    cluster_config = {\n",
    "        \"cluster_name\": cluster_name,\n",
    "        \"spark_version\": spark_version,\n",
    "        \"node_type_id\": node_type,\n",
    "        \"autoscale\": {\n",
    "            \"min_workers\": min_workers,\n",
    "            \"max_workers\": max_workers\n",
    "        },\n",
    "        \"custom_tags\": custom_tags\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        api_endpoint,\n",
    "        headers=headers,\n",
    "        data=json.dumps(cluster_config)\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"Successfully created cluster with ID: {result['cluster_id']}\")\n",
    "        return result['cluster_id']\n",
    "    else:\n",
    "        print(f\"Error creating cluster: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "cluster_id = create_cluster_with_tags(\n",
    "    cluster_name=CLUSTER_NAME,\n",
    "    custom_tags=TAGS_TO_ADD,\n",
    "    databricks_instance=DATABRICKS_INSTANCE,\n",
    "    token=TOKEN,\n",
    "    spark_version=SPARK_VERSION,\n",
    "    node_type=NODE_TYPE,\n",
    "    min_workers=MIN_WORKERS,\n",
    "    max_workers=MAX_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a01415f-6046-433d-96f9-810f34a8ae31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tagging SQL Warehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941213aa-562d-40d7-9abd-1e525c527de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "WAREHOUSE_ID = WAREHOUSE_ID\n",
    "DATABRICKS_INSTANCE = DATABRICKS_INSTANCE\n",
    "TOKEN = TOKEN\n",
    "TAGS_TO_ADD = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_sql_warehouse(warehouse_id, custom_tags, databricks_instance, token):\n",
    "    \"\"\"\n",
    "    Add or update tags on a Databricks SQL warehouse.\n",
    "    \n",
    "    Parameters:\n",
    "    - warehouse_id: ID of the SQL warehouse\n",
    "    - custom_tags: Dictionary of tag key-value pairs\n",
    "    - databricks_instance: Your Databricks workspace URL\n",
    "    - token: Your Databricks personal access token\n",
    "    \"\"\"\n",
    "    # First, get current warehouse configuration\n",
    "    api_endpoint = f\"https://{databricks_instance}/api/2.0/sql/warehouses/{warehouse_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        api_endpoint,\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error getting warehouse: {response.text}\")\n",
    "        return False\n",
    "    \n",
    "    # Get current configuration\n",
    "    warehouse_config = response.json()\n",
    "    \n",
    "    # Update the tags\n",
    "    current_tags = warehouse_config.get(\"tags\", {})\n",
    "    current_tags.update(custom_tags)\n",
    "    warehouse_config[\"tags\"] = current_tags\n",
    "    \n",
    "    # Edit warehouse with updated tags\n",
    "    edit_endpoint = f\"https://{databricks_instance}/api/2.0/sql/warehouses/{warehouse_id}/edit\"\n",
    "    \n",
    "    # Prepare the required fields for the edit request\n",
    "    edit_payload = {\n",
    "        \"id\": warehouse_id,\n",
    "        \"name\": warehouse_config[\"name\"],\n",
    "        \"tags\": current_tags\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        edit_endpoint,\n",
    "        headers=headers,\n",
    "        data=json.dumps(edit_payload)\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Successfully updated tags on SQL warehouse {warehouse_id}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Error updating tags: {response.text}\")\n",
    "        return False\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_sql_warehouse(\n",
    "    warehouse_id=WAREHOUSE_ID,\n",
    "    custom_tags=TAGS_TO_ADD,\n",
    "    databricks_instance=DATABRICKS_INSTANCE,\n",
    "    token=TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9195b068-290a-41ba-abd0-3b0cf2d3809a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unity Catalog Securable Object Tagging\n",
    "\n",
    "Unity Catalog allows you to apply tags to various data assets to improve organization, classification, governance, and discoverability.\n",
    "\n",
    "### Supported Securable Objects\n",
    "\n",
    "You can apply tags to the following objects in Unity Catalog:\n",
    "\n",
    "- Catalogs\n",
    "- Schemas\n",
    "- Tables (including views, materialized views, streaming tables)\n",
    "- Table columns\n",
    "- Volumes\n",
    "- Registered models and model versions\n",
    "\n",
    "\n",
    "### Tag Constraints and Requirements\n",
    "\n",
    "- Maximum of 50 tags per securable object\n",
    "- Maximum tag key length is 255 characters\n",
    "- Maximum tag value length is 1000 characters\n",
    "- Certain characters (`. , - = / :`) are not allowed in tag keys\n",
    "- Tag search requires exact term matching (no wildcards)\n",
    "- To add tags, you must own the object or have the `APPLY TAG` privilege, along with `USE SCHEMA` on the parent schema and `USE CATALOG` on the parent catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809fbbd6-3b4a-469b-857a-b9759f720ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"\n",
    "TABLE_NAME = \"diamonds\"\n",
    "TAGS_TO_APPLY = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_table(catalog_name, schema_name, table_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a table in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema\n",
    "    - table_name: Name of the table\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full table reference\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER TABLE {full_table_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to {full_table_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_table(\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    tags_dict=TAGS_TO_APPLY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c206992-8959-4115-8404-1ef7722daeed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tagging Table Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae865240-dbf6-4f32-9a34-35ac1ab0d45b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"\n",
    "TABLE_NAME = \"diamonds\"\n",
    "COLUMN_NAME = \"clarity\"\n",
    "TAGS_TO_APPLY = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_table_column(catalog_name, schema_name, table_name, column_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a specific column in a table.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema  \n",
    "    - table_name: Name of the table\n",
    "    - column_name: Name of the column to tag\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full table reference\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "    # SQL command to set column tags\n",
    "    sql_command = f\"ALTER TABLE {full_table_name} ALTER COLUMN {column_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to column {column_name} in {full_table_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_table_column(\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    column_name=COLUMN_NAME,\n",
    "    tags_dict=TAGS_TO_APPLY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d01f627-6cb9-45c1-b8b3-293c50914d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tagging Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97af973-9d15-44bb-b417-0b1172c5df81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"\n",
    "TAGS_TO_APPLY = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_schema(catalog_name, schema_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a schema in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full schema reference\n",
    "    full_schema_name = f\"{catalog_name}.{schema_name}\"\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER SCHEMA {full_schema_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to schema {full_schema_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_schema(\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    tags_dict=TAGS_TO_APPLY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80bfc1f8-8bfd-4b2b-968f-254bc721de56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tagging Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644821e2-f8ea-4058-a9b4-a2a231800b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "TAGS_TO_APPLY = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "\n",
    "def tag_catalog(catalog_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a catalog in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER CATALOG {catalog_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to catalog {catalog_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_catalog(\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    tags_dict=TAGS_TO_APPLY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be75be7-e0d7-453d-973c-5b0c690c094a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Removing Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d80e396-bd60-4e10-91cc-3db05f9561a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "SECURABLE_TYPE = \"TABLE\"  # Options: 'CATALOG', 'SCHEMA', 'TABLE', or 'COLUMN'\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"  # Set to None if not applicable\n",
    "TABLE_NAME = \"diamonds\"    # Set to None if not applicable\n",
    "COLUMN_NAME = \"clarity\"   # Set to None if not applicable\n",
    "TAG_KEYS = [\"project\", \"env\"]  # Set to None to remove all tags\n",
    "\n",
    "def remove_tags(securable_type, catalog_name, schema_name=None, table_name=None, \n",
    "               column_name=None, tag_keys=None):\n",
    "    \"\"\"\n",
    "    Remove tags from a securable object.\n",
    "    \n",
    "    Parameters:\n",
    "    - securable_type: Type of object ('CATALOG', 'SCHEMA', 'TABLE', or 'COLUMN')\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema (if applicable)\n",
    "    - table_name: Name of the table (if applicable)\n",
    "    - column_name: Name of the column (if applicable)\n",
    "    - tag_keys: List of tag keys to remove (if None, all tags are removed)\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    if securable_type == 'CATALOG':\n",
    "        object_name = catalog_name\n",
    "        sql_prefix = f\"ALTER CATALOG {object_name}\"\n",
    "    elif securable_type == 'SCHEMA':\n",
    "        object_name = f\"{catalog_name}.{schema_name}\"\n",
    "        sql_prefix = f\"ALTER SCHEMA {object_name}\"\n",
    "    elif securable_type == 'TABLE':\n",
    "        object_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "        sql_prefix = f\"ALTER TABLE {object_name}\"\n",
    "    elif securable_type == 'COLUMN':\n",
    "        object_name = f\"{catalog_name}.{schema_name}.{table_name}.{column_name}\"\n",
    "        sql_prefix = f\"ALTER TABLE {catalog_name}.{schema_name}.{table_name} ALTER COLUMN {column_name}\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid securable_type. Must be 'CATALOG', 'SCHEMA', 'TABLE', or 'COLUMN'\")\n",
    "    \n",
    "    # If tag_keys is provided, only remove those specific tags\n",
    "    if tag_keys:\n",
    "        tag_keys_sql = \", \".join([f\"'{key}'\" for key in tag_keys])\n",
    "        sql_command = f\"{sql_prefix} UNSET TAGS ({tag_keys_sql})\"\n",
    "    else:\n",
    "        # Get all existing tags for the object and remove them\n",
    "        if securable_type == 'CATALOG':\n",
    "            all_tags = spark.sql(f\"SELECT tag_name FROM system.information_schema.catalog_tags WHERE catalog_name = '{catalog_name}'\")\n",
    "        elif securable_type == 'SCHEMA':\n",
    "            all_tags = spark.sql(f\"SELECT tag_name FROM system.information_schema.schema_tags WHERE catalog_name = '{catalog_name}' AND schema_name = '{schema_name}'\")\n",
    "        elif securable_type == 'TABLE':\n",
    "            all_tags = spark.sql(f\"SELECT tag_name FROM system.information_schema.table_tags WHERE catalog_name = '{catalog_name}' AND schema_name = '{schema_name}' AND table_name = '{table_name}'\")\n",
    "        elif securable_type == 'COLUMN':\n",
    "            all_tags = spark.sql(f\"SELECT tag_name FROM system.information_schema.column_tags WHERE catalog_name = '{catalog_name}' AND schema_name = '{schema_name}' AND table_name = '{table_name}' AND column_name = '{column_name}'\")\n",
    "        \n",
    "        tag_keys = [row.tag_name for row in all_tags.collect()]\n",
    "        if tag_keys:\n",
    "            tag_keys_sql = \", \".join([f\"'{key}'\" for key in tag_keys])\n",
    "            sql_command = f\"{sql_prefix} UNSET TAGS ({tag_keys_sql})\"\n",
    "        else:\n",
    "            print(f\"No tags found for {object_name}\")\n",
    "            return\n",
    "    \n",
    "    # Execute the command\n",
    "    spark.sql(sql_command)\n",
    "    print(f\"Successfully removed tags from {object_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "remove_tags(\n",
    "    securable_type=SECURABLE_TYPE,\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    column_name=COLUMN_NAME,\n",
    "    tag_keys=TAG_KEYS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tagging_strategies",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
