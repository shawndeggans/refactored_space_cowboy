{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafcc9c5-125c-474b-a85e-d1088b4524ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Understanding Databricks Tagging Options\n",
    "\n",
    "Databricks provides three distinct tagging systems, each serving different purposes within the platform:\n",
    "\n",
    "1. **Resource-level tagging**: For attributing compute costs to teams, projects, or users\n",
    "2. **Unity Catalog securable object tagging**: For organizing, classifying, and governing data assets\n",
    "3. **Serverless compute workload tagging**: For tracking usage of serverless resources\n",
    "\n",
    "These tagging systems enable better governance, cost management, and organization of both compute resources and data assets within Databricks.\n",
    "\n",
    "## Resource-Level Tagging\n",
    "\n",
    "Resource-level tags allow you to attribute compute usage to specific teams, projects, or cost centers with greater granularity than default tags. These tags propagate to both your account's usage logs and applicable cloud resources.\n",
    "\n",
    "### Types of Resource Tags\n",
    "\n",
    "There are two types of resource tags in Databricks:\n",
    "\n",
    "1. **Default tags**: Automatically applied by Databricks to compute resources, providing basic metadata like vendor, cluster ID, and creator\n",
    "2. **Custom tags**: User-defined tags that you add to resources for more granular tracking\n",
    "\n",
    "### Supported Resources for Custom Tags\n",
    "\n",
    "You can apply custom tags to the following resources:\n",
    "\n",
    "\n",
    "| Object | Tagging interface | Python approach |\n",
    "| :-- | :-- | :-- |\n",
    "| Workspace | Cloud provider portal | Cloud provider API |\n",
    "| Pool | Pools UI or Instance Pool API | Databricks API |\n",
    "| Clusters (all-purpose and job) | Compute UI or Clusters API | Databricks API |\n",
    "| SQL warehouse | SQL warehouse UI or Warehouses API | Databricks API |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e299e93-c87a-4794-887d-a7cbac6ffa0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8234a52-82fe-430d-8b58-7ab6f6f71185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "    \n",
    "load_dotenv()\n",
    "\n",
    "TOKEN = os.getenv(\"TOKEN\")\n",
    "DATABRICKS_INSTANCE = os.getenv(\"DATABRICKS_INSTANCE\")\n",
    "CLUSTER_ID = os.getenv(\"CLUSTER_ID\")\n",
    "WAREHOUSE_ID = os.getenv(\"WAREHOUSE_ID\")\n",
    "\n",
    "print(f\"TOKEN: {TOKEN}\")\n",
    "print(f\"DATABRICKS_INSTANCE: {DATABRICKS_INSTANCE}\")\n",
    "print(f\"CLUSTER_ID: {CLUSTER_ID}\")\n",
    "print(f\"WAREHOUSE_ID: {WAREHOUSE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16db89f4-16db-405f-a6ec-8974d02ec0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Implementation with Python\n",
    "\n",
    "#### Tagging Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1145f7b3-3f02-4a8f-8af5-a99f0d4ad931",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CLUSTER_ID = CLUSTER_ID\n",
    "DATABRICKS_INSTANCE = DATABRICKS_INSTANCE\n",
    "TOKEN = TOKEN\n",
    "TAGS_TO_ADD = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_cluster(cluster_id, custom_tags, databricks_instance, token):\n",
    "    \"\"\"\n",
    "    Add or update tags on an existing Databricks cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_id: ID of the existing cluster\n",
    "    - custom_tags: Dictionary of tag key-value pairs\n",
    "    - databricks_instance: Your Databricks workspace URL\n",
    "    - token: Your Databricks personal access token\n",
    "    \"\"\"\n",
    "    # First, get current cluster configuration\n",
    "    api_endpoint = f\"https://{databricks_instance}/api/2.0/clusters/get\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        api_endpoint,\n",
    "        headers=headers,\n",
    "        params={\"cluster_id\": cluster_id}\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error getting cluster: {response.text}\")\n",
    "        return False\n",
    "    \n",
    "    # Get current configuration\n",
    "    cluster_config = response.json()\n",
    "    \n",
    "    # Update the tags\n",
    "    current_tags = cluster_config.get(\"custom_tags\", {})\n",
    "    current_tags.update(custom_tags)\n",
    "    cluster_config[\"custom_tags\"] = current_tags\n",
    "    \n",
    "    # Remove fields that cannot be included in edit request\n",
    "    for field in [\"creator_user_name\", \"start_time\", \"state\", \n",
    "                 \"state_message\", \"default_tags\", \"cluster_source\"]:\n",
    "        if field in cluster_config:\n",
    "            del cluster_config[field]\n",
    "\n",
    "    # Ensure cluster_id is included in the configuration\n",
    "    cluster_config[\"cluster_id\"] = cluster_id\n",
    "    \n",
    "    # Edit cluster with updated tags\n",
    "    edit_endpoint = f\"https://{databricks_instance}/api/2.0/clusters/edit\"\n",
    "    \n",
    "    response = requests.post(\n",
    "        edit_endpoint,\n",
    "        headers=headers,\n",
    "        data=json.dumps(cluster_config)\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Successfully updated tags on cluster {cluster_id}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Error updating tags: {response.text}\")\n",
    "        return False\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_cluster(\n",
    "    cluster_id=CLUSTER_ID,\n",
    "    custom_tags=TAGS_TO_ADD,\n",
    "    databricks_instance=DATABRICKS_INSTANCE,\n",
    "    token=TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fbd0743-e1ce-4a21-8e55-1fe6dec522b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Creating a New Cluster with Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3589f57d-7d73-415c-910d-a096f005f11d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CLUSTER_NAME = \"My Databricks Cluster\"\n",
    "DATABRICKS_INSTANCE = DATABRICKS_INSTANCE\n",
    "TOKEN = TOKEN\n",
    "SPARK_VERSION = \"11.3.x-scala2.12\"\n",
    "NODE_TYPE = \"Standard_DS3_v2\"\n",
    "MIN_WORKERS = 1\n",
    "MAX_WORKERS = 2\n",
    "TAGS_TO_ADD = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def create_cluster_with_tags(cluster_name, custom_tags, databricks_instance, token, \n",
    "                            spark_version=\"11.3.x-scala2.12\", node_type=\"Standard_DS3_v2\", \n",
    "                            min_workers=1, max_workers=2):\n",
    "    \"\"\"\n",
    "    Create a new Databricks cluster with custom tags.\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_name: Name for the new cluster\n",
    "    - custom_tags: Dictionary of tag key-value pairs\n",
    "    - databricks_instance: Your Databricks workspace URL\n",
    "    - token: Your Databricks personal access token\n",
    "    - spark_version: Databricks Runtime version\n",
    "    - node_type: VM type for the cluster nodes\n",
    "    - min_workers, max_workers: Worker node count range\n",
    "    \"\"\"\n",
    "    api_endpoint = f\"https://{databricks_instance}/api/2.0/clusters/create\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # IMPORTANT: Don't use 'Name' as a custom tag key (reserved by Databricks)\n",
    "    if \"Name\" in custom_tags:\n",
    "        print(\"Warning: 'Name' is a reserved tag and has been removed\")\n",
    "        del custom_tags[\"Name\"]\n",
    "    \n",
    "    cluster_config = {\n",
    "        \"cluster_name\": cluster_name,\n",
    "        \"spark_version\": spark_version,\n",
    "        \"node_type_id\": node_type,\n",
    "        \"autoscale\": {\n",
    "            \"min_workers\": min_workers,\n",
    "            \"max_workers\": max_workers\n",
    "        },\n",
    "        \"custom_tags\": custom_tags\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        api_endpoint,\n",
    "        headers=headers,\n",
    "        data=json.dumps(cluster_config)\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"Successfully created cluster with ID: {result['cluster_id']}\")\n",
    "        return result['cluster_id']\n",
    "    else:\n",
    "        print(f\"Error creating cluster: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "cluster_id = create_cluster_with_tags(\n",
    "    cluster_name=CLUSTER_NAME,\n",
    "    custom_tags=TAGS_TO_ADD,\n",
    "    databricks_instance=DATABRICKS_INSTANCE,\n",
    "    token=TOKEN,\n",
    "    spark_version=SPARK_VERSION,\n",
    "    node_type=NODE_TYPE,\n",
    "    min_workers=MIN_WORKERS,\n",
    "    max_workers=MAX_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a01415f-6046-433d-96f9-810f34a8ae31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tagging SQL Warehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "941213aa-562d-40d7-9abd-1e525c527de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "WAREHOUSE_ID = WAREHOUSE_ID\n",
    "DATABRICKS_INSTANCE = DATABRICKS_INSTANCE\n",
    "TOKEN = TOKEN\n",
    "TAGS_TO_ADD = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_sql_warehouse(warehouse_id, custom_tags, databricks_instance, token):\n",
    "    \"\"\"\n",
    "    Add or update tags on a Databricks SQL warehouse.\n",
    "    \n",
    "    Parameters:\n",
    "    - warehouse_id: ID of the SQL warehouse\n",
    "    - custom_tags: Dictionary of tag key-value pairs\n",
    "    - databricks_instance: Your Databricks workspace URL\n",
    "    - token: Your Databricks personal access token\n",
    "    \"\"\"\n",
    "    # First, get current warehouse configuration\n",
    "    api_endpoint = f\"https://{databricks_instance}/api/2.0/sql/warehouses/{warehouse_id}\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        api_endpoint,\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error getting warehouse: {response.text}\")\n",
    "        return False\n",
    "    \n",
    "    # Get current configuration\n",
    "    warehouse_config = response.json()\n",
    "    \n",
    "    # Update the tags\n",
    "    current_tags = warehouse_config.get(\"tags\", {})\n",
    "    current_tags.update(custom_tags)\n",
    "    warehouse_config[\"tags\"] = current_tags\n",
    "    \n",
    "    # Edit warehouse with updated tags\n",
    "    edit_endpoint = f\"https://{databricks_instance}/api/2.0/sql/warehouses/{warehouse_id}/edit\"\n",
    "    \n",
    "    # Prepare the required fields for the edit request\n",
    "    edit_payload = {\n",
    "        \"id\": warehouse_id,\n",
    "        \"name\": warehouse_config[\"name\"],\n",
    "        \"tags\": current_tags\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        edit_endpoint,\n",
    "        headers=headers,\n",
    "        data=json.dumps(edit_payload)\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Successfully updated tags on SQL warehouse {warehouse_id}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Error updating tags: {response.text}\")\n",
    "        return False\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_sql_warehouse(\n",
    "    warehouse_id=WAREHOUSE_ID,\n",
    "    custom_tags=TAGS_TO_ADD,\n",
    "    databricks_instance=DATABRICKS_INSTANCE,\n",
    "    token=TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9195b068-290a-41ba-abd0-3b0cf2d3809a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Unity Catalog Securable Object Tagging\n",
    "\n",
    "Unity Catalog allows you to apply tags to various data assets to improve organization, classification, governance, and discoverability.\n",
    "\n",
    "### Supported Securable Objects\n",
    "\n",
    "You can apply tags to the following objects in Unity Catalog:\n",
    "\n",
    "- Catalogs\n",
    "- Schemas\n",
    "- Tables (including views, materialized views, streaming tables)\n",
    "- Table columns\n",
    "- Volumes\n",
    "- Registered models and model versions\n",
    "\n",
    "\n",
    "### Tag Constraints and Requirements\n",
    "\n",
    "- Maximum of 50 tags per securable object\n",
    "- Maximum tag key length is 255 characters\n",
    "- Maximum tag value length is 1000 characters\n",
    "- Certain characters (`. , - = / :`) are not allowed in tag keys\n",
    "- Tag search requires exact term matching (no wildcards)\n",
    "- To add tags, you must own the object or have the `APPLY TAG` privilege, along with `USE SCHEMA` on the parent schema and `USE CATALOG` on the parent catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809fbbd6-3b4a-469b-857a-b9759f720ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"\n",
    "TABLE_NAME = \"diamonds\"\n",
    "TAGS_TO_APPLY = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_table(catalog_name, schema_name, table_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a table in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema\n",
    "    - table_name: Name of the table\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full table reference\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER TABLE {full_table_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to {full_table_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_table(\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    tags_dict=TAGS_TO_APPLY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c206992-8959-4115-8404-1ef7722daeed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tagging Table Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae865240-dbf6-4f32-9a34-35ac1ab0d45b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"\n",
    "TABLE_NAME = \"diamonds\"\n",
    "COLUMN_NAME = \"clarity\"\n",
    "TAGS_TO_APPLY = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_table_column(catalog_name, schema_name, table_name, column_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a specific column in a table.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema  \n",
    "    - table_name: Name of the table\n",
    "    - column_name: Name of the column to tag\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full table reference\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "    # SQL command to set column tags\n",
    "    sql_command = f\"ALTER TABLE {full_table_name} ALTER COLUMN {column_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to column {column_name} in {full_table_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_table_column(\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    column_name=COLUMN_NAME,\n",
    "    tags_dict=TAGS_TO_APPLY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d01f627-6cb9-45c1-b8b3-293c50914d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tagging Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97af973-9d15-44bb-b417-0b1172c5df81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"\n",
    "TAGS_TO_APPLY = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "def tag_schema(catalog_name, schema_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a schema in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full schema reference\n",
    "    full_schema_name = f\"{catalog_name}.{schema_name}\"\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER SCHEMA {full_schema_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to schema {full_schema_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_schema(\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    tags_dict=TAGS_TO_APPLY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80bfc1f8-8bfd-4b2b-968f-254bc721de56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Tagging Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644821e2-f8ea-4058-a9b4-a2a231800b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "TAGS_TO_APPLY = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "    \"project\": \"dipt\",\n",
    "    \"env\": \"prod\"\n",
    "}\n",
    "\n",
    "\n",
    "def tag_catalog(catalog_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a catalog in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER CATALOG {catalog_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to catalog {catalog_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "tag_catalog(\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    tags_dict=TAGS_TO_APPLY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be75be7-e0d7-453d-973c-5b0c690c094a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Removing Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d80e396-bd60-4e10-91cc-3db05f9561a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "SECURABLE_TYPE = \"TABLE\"  # Options: 'CATALOG', 'SCHEMA', 'TABLE', or 'COLUMN'\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"  # Set to None if not applicable\n",
    "TABLE_NAME = \"diamonds\"    # Set to None if not applicable\n",
    "COLUMN_NAME = \"clarity\"   # Set to None if not applicable\n",
    "TAG_KEYS = [\"project\", \"env\"]  # Set to None to remove all tags\n",
    "\n",
    "def remove_tags(securable_type, catalog_name, schema_name=None, table_name=None, \n",
    "               column_name=None, tag_keys=None):\n",
    "    \"\"\"\n",
    "    Remove tags from a securable object.\n",
    "    \n",
    "    Parameters:\n",
    "    - securable_type: Type of object ('CATALOG', 'SCHEMA', 'TABLE', or 'COLUMN')\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema (if applicable)\n",
    "    - table_name: Name of the table (if applicable)\n",
    "    - column_name: Name of the column (if applicable)\n",
    "    - tag_keys: List of tag keys to remove (if None, all tags are removed)\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    if securable_type == 'CATALOG':\n",
    "        object_name = catalog_name\n",
    "        sql_prefix = f\"ALTER CATALOG {object_name}\"\n",
    "    elif securable_type == 'SCHEMA':\n",
    "        object_name = f\"{catalog_name}.{schema_name}\"\n",
    "        sql_prefix = f\"ALTER SCHEMA {object_name}\"\n",
    "    elif securable_type == 'TABLE':\n",
    "        object_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "        sql_prefix = f\"ALTER TABLE {object_name}\"\n",
    "    elif securable_type == 'COLUMN':\n",
    "        object_name = f\"{catalog_name}.{schema_name}.{table_name}.{column_name}\"\n",
    "        sql_prefix = f\"ALTER TABLE {catalog_name}.{schema_name}.{table_name} ALTER COLUMN {column_name}\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid securable_type. Must be 'CATALOG', 'SCHEMA', 'TABLE', or 'COLUMN'\")\n",
    "    \n",
    "    # If tag_keys is provided, only remove those specific tags\n",
    "    if tag_keys:\n",
    "        tag_keys_sql = \", \".join([f\"'{key}'\" for key in tag_keys])\n",
    "        sql_command = f\"{sql_prefix} UNSET TAGS ({tag_keys_sql})\"\n",
    "    else:\n",
    "        # Get all existing tags for the object and remove them\n",
    "        if securable_type == 'CATALOG':\n",
    "            all_tags = spark.sql(f\"SELECT tag_name FROM system.information_schema.catalog_tags WHERE catalog_name = '{catalog_name}'\")\n",
    "        elif securable_type == 'SCHEMA':\n",
    "            all_tags = spark.sql(f\"SELECT tag_name FROM system.information_schema.schema_tags WHERE catalog_name = '{catalog_name}' AND schema_name = '{schema_name}'\")\n",
    "        elif securable_type == 'TABLE':\n",
    "            all_tags = spark.sql(f\"SELECT tag_name FROM system.information_schema.table_tags WHERE catalog_name = '{catalog_name}' AND schema_name = '{schema_name}' AND table_name = '{table_name}'\")\n",
    "        elif securable_type == 'COLUMN':\n",
    "            all_tags = spark.sql(f\"SELECT tag_name FROM system.information_schema.column_tags WHERE catalog_name = '{catalog_name}' AND schema_name = '{schema_name}' AND table_name = '{table_name}' AND column_name = '{column_name}'\")\n",
    "        \n",
    "        tag_keys = [row.tag_name for row in all_tags.collect()]\n",
    "        if tag_keys:\n",
    "            tag_keys_sql = \", \".join([f\"'{key}'\" for key in tag_keys])\n",
    "            sql_command = f\"{sql_prefix} UNSET TAGS ({tag_keys_sql})\"\n",
    "        else:\n",
    "            print(f\"No tags found for {object_name}\")\n",
    "            return\n",
    "    \n",
    "    # Execute the command\n",
    "    spark.sql(sql_command)\n",
    "    print(f\"Successfully removed tags from {object_name}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "remove_tags(\n",
    "    securable_type=SECURABLE_TYPE,\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    column_name=COLUMN_NAME,\n",
    "    tag_keys=TAG_KEYS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "684c8efa-c05b-4217-9130-4c5f31cb763b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Automated Tagging Strategies\n",
    "#### Automated Table and Column Tagging Based on Content\n",
    "\n",
    "This example shows how to automatically scan tables for sensitive data patterns and apply appropriate tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d63d463b-c92b-4495-80b3-7f95210635c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import re\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"\n",
    "TABLE_NAME = \"diamonds\"\n",
    "\n",
    "# PII patterns configuration - customize as needed\n",
    "PII_PATTERNS = {\n",
    "    \"EMAIL\": r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$',\n",
    "    \"SSN\": r'^\\d{3}-\\d{2}-\\d{4}$',\n",
    "    \"CREDIT_CARD\": r'^\\d{4}-\\d{4}-\\d{4}-\\d{4}$',\n",
    "    \"PHONE_NUMBER\": r'^\\d{3}-\\d{3}-\\d{4}$'\n",
    "}\n",
    "\n",
    "# PII name indicators configuration - customize as needed\n",
    "PII_NAME_INDICATORS = {\n",
    "    \"EMAIL\": [\"email\", \"e-mail\", \"mail\"],\n",
    "    \"SSN\": [\"ssn\", \"social\", \"security\"],\n",
    "    \"CREDIT_CARD\": [\"cc\", \"credit\", \"card\", \"payment\"],\n",
    "    \"PHONE\": [\"phone\", \"mobile\", \"cell\", \"tel\"],\n",
    "    \"ADDRESS\": [\"address\", \"addr\", \"street\"],\n",
    "    \"NAME\": [\"name\", \"firstname\", \"lastname\", \"fullname\"]\n",
    "}\n",
    "\n",
    "# Function to tag table columns\n",
    "def tag_table_column(catalog_name, schema_name, table_name, column_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a specific column in a table.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema  \n",
    "    - table_name: Name of the table\n",
    "    - column_name: Name of the column to tag\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full table reference\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "    # SQL command to set column tags\n",
    "    sql_command = f\"ALTER TABLE {full_table_name} ALTER COLUMN {column_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to column {column_name} in {full_table_name}\")\n",
    "\n",
    "# Function to tag tables\n",
    "def tag_table(catalog_name, schema_name, table_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a table in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema\n",
    "    - table_name: Name of the table\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full table reference\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER TABLE {full_table_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to {full_table_name}\")\n",
    "\n",
    "def scan_and_tag_pii_columns(catalog_name, schema_name, table_name, pii_patterns, pii_name_indicators):\n",
    "    \"\"\"\n",
    "    Scan a table for potential PII columns and apply appropriate tags.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema\n",
    "    - table_name: Name of the table\n",
    "    - pii_patterns: Dictionary of regex patterns for PII detection\n",
    "    - pii_name_indicators: Dictionary of keywords that suggest PII in column names\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Get table schema\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    df = spark.table(full_table_name)\n",
    "    \n",
    "    # For each column, check for PII patterns\n",
    "    for column in df.columns:\n",
    "        col_data = df.select(col(column)).limit(100)  # Sample first 100 rows\n",
    "        \n",
    "        # Skip if column is not string type\n",
    "        if str(df.schema[column].dataType) not in ['StringType', 'VarcharType', 'CharType']:\n",
    "            continue\n",
    "            \n",
    "        detected_tags = {}\n",
    "        \n",
    "        # Check column name for PII indicators\n",
    "        for pii_type, indicators in pii_name_indicators.items():\n",
    "            if any(indicator in column.lower() for indicator in indicators):\n",
    "                detected_tags[\"PII_TYPE\"] = pii_type\n",
    "                detected_tags[\"SENSITIVITY\"] = \"HIGH\"\n",
    "                break\n",
    "                \n",
    "        # Check sample data for PII patterns\n",
    "        if not detected_tags:\n",
    "            for pii_type, pattern in pii_patterns.items():\n",
    "                # Convert column to string and check if any values match the pattern\n",
    "                matches = col_data.filter(col(column).rlike(pattern)).count()\n",
    "                if matches > 0:\n",
    "                    detected_tags[\"PII_TYPE\"] = pii_type\n",
    "                    detected_tags[\"SENSITIVITY\"] = \"HIGH\"\n",
    "                    break\n",
    "        \n",
    "        # Apply tags if PII was detected\n",
    "        if detected_tags:\n",
    "            tag_table_column(catalog_name, schema_name, table_name, column, detected_tags)\n",
    "            print(f\"Tagged column {column} in {full_table_name} as {detected_tags}\")\n",
    "    \n",
    "    # Apply table-level tags based on column findings\n",
    "    table_tags = {}\n",
    "    \n",
    "    # Query information schema to see what PII was found\n",
    "    query = f\"\"\"\n",
    "    SELECT tag_name, tag_value \n",
    "    FROM system.information_schema.column_tags \n",
    "    WHERE catalog_name = '{catalog_name}' \n",
    "    AND schema_name = '{schema_name}' \n",
    "    AND table_name = '{table_name}'\n",
    "    AND tag_name = 'PII_TYPE'\n",
    "    \"\"\"\n",
    "    \n",
    "    pii_columns = spark.sql(query).collect()\n",
    "    \n",
    "    if pii_columns:\n",
    "        table_tags[\"CONTAINS_PII\"] = \"TRUE\"\n",
    "        table_tags[\"GOVERNANCE_LEVEL\"] = \"RESTRICTED\"\n",
    "        \n",
    "        # Apply the table-level tags\n",
    "        tag_table(catalog_name, schema_name, table_name, table_tags)\n",
    "        print(f\"Tagged table {full_table_name} as {table_tags}\")\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "scan_and_tag_pii_columns(\n",
    "    catalog_name=CATALOG_NAME,\n",
    "    schema_name=SCHEMA_NAME,\n",
    "    table_name=TABLE_NAME,\n",
    "    pii_patterns=PII_PATTERNS,\n",
    "    pii_name_indicators=PII_NAME_INDICATORS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "192fcf73-ddc1-4aa3-b801-c84f797965d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Bulk Tagging Using Configuration Files\n",
    "\n",
    "This example shows how to apply tags to multiple objects using a YAML configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3004b954-3556-42cb-802b-39049d084804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "CONFIG_FILE_PATH = \"tags_config.yaml\"\n",
    "\n",
    "# Sample YAML configuration content - can be used to create a config file\n",
    "SAMPLE_CONFIG = \"\"\"\n",
    "catalogs:\n",
    "  - name: main\n",
    "    tags:\n",
    "      Environment: Production\n",
    "      Department: Engineering\n",
    "      \n",
    "schemas:\n",
    "  - catalog: main\n",
    "    name: sales\n",
    "    tags:\n",
    "      DataDomain: Revenue\n",
    "      Owner: Finance\n",
    "      \n",
    "tables:\n",
    "  - catalog: main\n",
    "    schema: sales\n",
    "    name: transactions\n",
    "    tags:\n",
    "      UpdateFrequency: Daily\n",
    "      Retention: 5Years\n",
    "      BusinessCritical: True\n",
    "    columns:\n",
    "      - name: customer_id\n",
    "        tags:\n",
    "          PII_TYPE: IDENTIFIER\n",
    "          JOIN_KEY: True\n",
    "      - name: email\n",
    "        tags:\n",
    "          PII_TYPE: EMAIL\n",
    "          SENSITIVITY: HIGH\n",
    "\"\"\"\n",
    "\n",
    "# Helper functions for tagging different objects\n",
    "def tag_catalog(catalog_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a catalog in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER CATALOG {catalog_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to catalog {catalog_name}\")\n",
    "\n",
    "def tag_schema(catalog_name, schema_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a schema in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full schema reference\n",
    "    full_schema_name = f\"{catalog_name}.{schema_name}\"\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER SCHEMA {full_schema_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to schema {full_schema_name}\")\n",
    "\n",
    "def tag_table(catalog_name, schema_name, table_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a table in Unity Catalog.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema\n",
    "    - table_name: Name of the table\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full table reference\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "    # SQL command to set tags\n",
    "    sql_command = f\"ALTER TABLE {full_table_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to {full_table_name}\")\n",
    "\n",
    "def tag_table_column(catalog_name, schema_name, table_name, column_name, tags_dict):\n",
    "    \"\"\"\n",
    "    Apply tags to a specific column in a table.\n",
    "    \n",
    "    Parameters:\n",
    "    - catalog_name: Name of the catalog\n",
    "    - schema_name: Name of the schema  \n",
    "    - table_name: Name of the table\n",
    "    - column_name: Name of the column to tag\n",
    "    - tags_dict: Dictionary of tag key-value pairs\n",
    "    \"\"\"\n",
    "    # Convert tags dictionary to SQL format\n",
    "    tags_sql = \", \".join([f\"'{k}' = '{v}'\" for k, v in tags_dict.items()])\n",
    "    \n",
    "    # Full table reference\n",
    "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "    \n",
    "    # SQL command to set column tags\n",
    "    sql_command = f\"ALTER TABLE {full_table_name} ALTER COLUMN {column_name} SET TAGS ({tags_sql})\"\n",
    "    \n",
    "    # Execute the SQL command\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sql(sql_command)\n",
    "    \n",
    "    print(f\"Successfully applied tags to column {column_name} in {full_table_name}\")\n",
    "\n",
    "def apply_tags_from_config(config_file_path):\n",
    "    \"\"\"\n",
    "    Apply tags to multiple objects from a YAML configuration file.\n",
    "    \n",
    "    Parameters:\n",
    "    - config_file_path: Path to the YAML configuration file\n",
    "    \"\"\"\n",
    "    # Load configuration\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Process each object type\n",
    "    for object_type, objects in config.items():\n",
    "        if object_type == 'catalogs':\n",
    "            for catalog in objects:\n",
    "                name = catalog['name']\n",
    "                tags = catalog.get('tags', {})\n",
    "                if tags:\n",
    "                    tag_catalog(name, tags)\n",
    "        \n",
    "        elif object_type == 'schemas':\n",
    "            for schema in objects:\n",
    "                catalog = schema['catalog']\n",
    "                name = schema['name']\n",
    "                tags = schema.get('tags', {})\n",
    "                if tags:\n",
    "                    tag_schema(catalog, name, tags)\n",
    "        \n",
    "        elif object_type == 'tables':\n",
    "            for table in objects:\n",
    "                catalog = table['catalog']\n",
    "                schema = table['schema']\n",
    "                name = table['name']\n",
    "                tags = table.get('tags', {})\n",
    "                if tags:\n",
    "                    tag_table(catalog, schema, name, tags)\n",
    "                \n",
    "                # Process column tags if present\n",
    "                columns = table.get('columns', [])\n",
    "                for column in columns:\n",
    "                    column_name = column['name']\n",
    "                    column_tags = column.get('tags', {})\n",
    "                    if column_tags:\n",
    "                        tag_table_column(catalog, schema, name, column_name, column_tags)\n",
    "\n",
    "# Create a sample config file (optional)\n",
    "def create_sample_config(file_path):\n",
    "    \"\"\"Create a sample YAML configuration file\"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(SAMPLE_CONFIG)\n",
    "    print(f\"Sample configuration file created at {file_path}\")\n",
    "\n",
    "# Uncomment to create a sample config file\n",
    "create_sample_config(CONFIG_FILE_PATH)\n",
    "\n",
    "# Call the function with the parameter defined at the top\n",
    "apply_tags_from_config(config_file_path=CONFIG_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b973c0a-2690-4a89-b6de-b915ff383c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Serverless Compute Workload Tagging\n",
    "\n",
    "To attribute serverless compute usage, Databricks uses serverless budget policies. This feature is in Public Preview and allows you to tag serverless notebooks, jobs, pipelines, and model serving endpoints.\n",
    "\n",
    "Since this requires administrative setup through the Databricks account console, we'll describe the general approach:\n",
    "\n",
    "- Administrator creates serverless budget policies with custom tags\n",
    "- Users or user groups are assigned to these policies\n",
    "- Any serverless usage by these users is automatically tagged with the policy's custom tags\n",
    "\n",
    "While this can't be directly implemented with Python code, you can query the tagged usage data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f764919f-56e8-4588-8445-faf4c97492e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Parameters - set these variables at the top of your notebook cell\n",
    "START_DATE = \"2025-01-01\"  # Format: YYYY-MM-DD\n",
    "END_DATE = \"2025-01-31\"    # Format: YYYY-MM-DD\n",
    "FILTER_TAGS = {\n",
    "    \"role\": \"data_science\",\n",
    "    \"req\": \"4356\",\n",
    "}  # Set to None to query all serverless usage\n",
    "\n",
    "# Catalog, schema, and table for your sample data \n",
    "CATALOG_NAME = \"tagging_test\"\n",
    "SCHEMA_NAME = \"tagging_tables\"\n",
    "TABLE_NAME = \"diamonds\"\n",
    "\n",
    "def query_tagged_serverless_usage(start_date, end_date, filter_tags=None):\n",
    "    \"\"\"\n",
    "    Query serverless usage data with specific tags.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_date: Start date for usage query (YYYY-MM-DD)\n",
    "    - end_date: End date for usage query (YYYY-MM-DD)\n",
    "    - filter_tags: Dictionary of tags to filter by (optional)\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # Build query for the billable usage system table\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        workspace_id,\n",
    "        record_id,\n",
    "        usage_date,\n",
    "        sku_name,\n",
    "        usage_type,\n",
    "        usage_quantity,\n",
    "        custom_tags\n",
    "    FROM \n",
    "        system.billing.usage  -- or whatever the correct table name is\n",
    "    WHERE \n",
    "        usage_date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        AND usage_type = 'COMPUTE_TIME'\n",
    "\"\"\"\n",
    "    \n",
    "    # Add tag filters if provided\n",
    "    if filter_tags:\n",
    "        for key, value in filter_tags.items():\n",
    "            query += f\" AND custom_tags['{key}'] = '{value}'\"\n",
    "    \n",
    "    # Execute query\n",
    "    usage_data = spark.sql(query)\n",
    "    \n",
    "    return usage_data\n",
    "\n",
    "# Function to view sample data\n",
    "def view_sample_data():\n",
    "    \"\"\"View the sample diamonds data to verify it exists\"\"\"\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "    \n",
    "    try:\n",
    "        df = spark.table(full_table_name)\n",
    "        print(f\"Sample data from {full_table_name}:\")\n",
    "        df.show(5)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing table {full_table_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Call the function with the parameters defined at the top\n",
    "usage_data = query_tagged_serverless_usage(\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    filter_tags=FILTER_TAGS\n",
    ")\n",
    "\n",
    "# Uncomment to display results\n",
    "# display(usage_data)\n",
    "\n",
    "# Uncomment to view your sample diamonds data\n",
    "# view_sample_data()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tagging_strategies",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
